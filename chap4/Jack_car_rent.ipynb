{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e03e914f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import exp, factorial\n",
    "\n",
    "MAX_CARS = 20\n",
    "MAX_MOVE = 5\n",
    "\n",
    "RENT_REWARD = 10\n",
    "MOVE_COST = 2\n",
    "DISCOUNT = 0.9\n",
    "\n",
    "# Poisson parameters\n",
    "LAMBDA_REQ_1 = 3\n",
    "LAMBDA_REQ_2 = 4\n",
    "LAMBDA_RET_1 = 3\n",
    "LAMBDA_RET_2 = 2\n",
    "\n",
    "# Exercise 4.7 modifications\n",
    "FREE_MOVE_1_TO_2 = 1\n",
    "PARKING_LIMIT = 10\n",
    "PARKING_COST = 4\n",
    "\n",
    "POISSON_MAX = 11   # truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81468836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson_pmf(lam, n):\n",
    "    return (lam ** n) * exp(-lam) / factorial(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5e904d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson_probs(lam, max_n=POISSON_MAX):\n",
    "    probs = np.zeros(max_n + 1)\n",
    "    for n in range(max_n):\n",
    "        probs[n] = poisson_pmf(lam,n)\n",
    "        \n",
    "    probs[max_n] = 1 - probs[:max_n].sum()\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0a347fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_REQ_1 = poisson_probs(LAMBDA_REQ_1)\n",
    "P_REQ_2 = poisson_probs(LAMBDA_REQ_2)\n",
    "P_RET_1 = poisson_probs(LAMBDA_RET_1)\n",
    "P_RET_2 = poisson_probs(LAMBDA_RET_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0832224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_actions(state):\n",
    "    c1, c2 = state\n",
    "    actions = []\n",
    "    for a in range(-MAX_MOVE, MAX_MOVE + 1):\n",
    "        if a < 0:\n",
    "            move = -a\n",
    "            if move <= c2 and c1 + move <= MAX_CARS:\n",
    "                actions.append(a)\n",
    "        else:\n",
    "            move = a\n",
    "            if move <= c1 and c2 + move <= MAX_CARS:\n",
    "                actions.append(a)\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49539343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_cost(a):\n",
    "    if a > 0:\n",
    "        paid = max(a - FREE_MOVE_1_TO_2, 0)\n",
    "        return MOVE_COST * paid\n",
    "    else:\n",
    "        return MOVE_COST * abs(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "716eeb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parking_cost(c1_after_move, c2_after_move):\n",
    "    cost = 0\n",
    "    if c1_after_move > PARKING_LIMIT:\n",
    "        cost += PARKING_COST\n",
    "    if c2_after_move > PARKING_LIMIT:\n",
    "        cost += PARKING_COST\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c7bc33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition(state, action, req1, req2, ret1, ret2):\n",
    "    c1, c2 = state\n",
    "    c1_m = c1 - action\n",
    "    c2_m = c2 + action\n",
    "    \n",
    "    cost_move = moving_cost(action)\n",
    "    cost_park = parking_cost(c1_m, c2_m)\n",
    "    \n",
    "    rent1 = min(c1_m, req1)\n",
    "    rent2 = min(c2_m, req2)\n",
    "    \n",
    "    reward = RENT_REWARD * (rent1 + rent2)\n",
    "    reward -= cost_move + cost_park\n",
    "    \n",
    "    c1_after = c1_m - rent1\n",
    "    c2_after = c2_m - rent2\n",
    "    \n",
    "    next_c1 = min(c1_after + ret1, MAX_CARS)\n",
    "    next_c2 = min(c2_after + ret2, MAX_CARS)\n",
    "    \n",
    "    return (next_c1, next_c2), reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82f5d8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_return(state, action, V):\n",
    "\n",
    "    total = 0.0\n",
    "\n",
    "    for req1 in range(POISSON_MAX + 1):\n",
    "        for req2 in range(POISSON_MAX + 1):\n",
    "            for ret1 in range(POISSON_MAX + 1):\n",
    "                for ret2 in range(POISSON_MAX + 1):\n",
    "\n",
    "                    prob = P_REQ_1[req1] * P_REQ_2[req2] * P_RET_1[ret1] * P_RET_2[ret2]\n",
    "\n",
    "                    next_state, reward = transition(\n",
    "                        state, action, req1, req2, ret1, ret2\n",
    "                    )\n",
    "\n",
    "                    total += prob * (reward + DISCOUNT * V[next_state])\n",
    "\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16fadc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, V, theta=1e-3):\n",
    "\n",
    "    deltas = []  # store max change per sweep\n",
    "\n",
    "    while True:\n",
    "        delta = 0.0\n",
    "\n",
    "        for c1 in range(MAX_CARS + 1):\n",
    "            for c2 in range(MAX_CARS + 1):\n",
    "\n",
    "                s = (c1, c2)\n",
    "                v_old = V[s]\n",
    "\n",
    "                a = policy[s]\n",
    "                V[s] = expected_return(s, a, V)\n",
    "\n",
    "                delta = max(delta, abs(v_old - V[s]))\n",
    "\n",
    "        deltas.append(delta)\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return V, deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a411c367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(policy, V):\n",
    "\n",
    "    stable = True\n",
    "\n",
    "    for c1 in range(MAX_CARS + 1):\n",
    "        for c2 in range(MAX_CARS + 1):\n",
    "\n",
    "            s = (c1, c2)\n",
    "            old_action = policy[s]\n",
    "\n",
    "            actions = valid_actions(s)\n",
    "            values = [expected_return(s, a, V) for a in actions]\n",
    "\n",
    "            best_action = actions[np.argmax(values)]\n",
    "            policy[s] = best_action\n",
    "\n",
    "            if best_action != old_action:\n",
    "                stable = False\n",
    "\n",
    "    return policy, stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96def1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration_with_tracking():\n",
    "\n",
    "    V = np.zeros((MAX_CARS + 1, MAX_CARS + 1))\n",
    "    policy = np.zeros((MAX_CARS + 1, MAX_CARS + 1), dtype=int)\n",
    "\n",
    "    all_deltas = []\n",
    "    policy_returns = []\n",
    "\n",
    "    while True:\n",
    "\n",
    "        V, deltas = policy_evaluation(policy, V)\n",
    "        all_deltas.extend(deltas)\n",
    "\n",
    "        # total expected value under policy (sum over states)\n",
    "        policy_returns.append(np.sum(V))\n",
    "\n",
    "        policy, stable = policy_improvement(policy, V)\n",
    "\n",
    "        if stable:\n",
    "            break\n",
    "\n",
    "    return policy, V, all_deltas, policy_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76cd9ead",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m policy, V, deltas, returns = \u001b[43mpolicy_iteration_with_tracking\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# ---- Plot convergence (Bellman error) ----\u001b[39;00m\n\u001b[32m      6\u001b[39m plt.figure()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mpolicy_iteration_with_tracking\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      7\u001b[39m policy_returns = []\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     V, deltas = \u001b[43mpolicy_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     all_deltas.extend(deltas)\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# total expected value under policy (sum over states)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mpolicy_evaluation\u001b[39m\u001b[34m(policy, V, theta)\u001b[39m\n\u001b[32m     12\u001b[39m         v_old = V[s]\n\u001b[32m     14\u001b[39m         a = policy[s]\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m         V[s] = \u001b[43mexpected_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m         delta = \u001b[38;5;28mmax\u001b[39m(delta, \u001b[38;5;28mabs\u001b[39m(v_old - V[s]))\n\u001b[32m     19\u001b[39m deltas.append(delta)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mexpected_return\u001b[39m\u001b[34m(state, action, V)\u001b[39m\n\u001b[32m      8\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m ret2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(POISSON_MAX + \u001b[32m1\u001b[39m):\n\u001b[32m     10\u001b[39m                 prob = P_REQ_1[req1] * P_REQ_2[req2] * P_RET_1[ret1] * P_RET_2[ret2]\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m                 next_state, reward = \u001b[43mtransition\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mret1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mret2\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m                 total += prob * (reward + DISCOUNT * V[next_state])\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m total\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m, in \u001b[36mtransition\u001b[39m\u001b[34m(state, action, req1, req2, ret1, ret2)\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtransition\u001b[39m(state, action, req1, req2, ret1, ret2):\n\u001b[32m      2\u001b[39m     c1, c2 = state\n\u001b[32m      3\u001b[39m     c1_m = c1 - action\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "policy, V, deltas, returns = policy_iteration_with_tracking()\n",
    "\n",
    "# ---- Plot convergence (Bellman error) ----\n",
    "plt.figure()\n",
    "plt.plot(deltas)\n",
    "plt.xlabel(\"Evaluation Sweep\")\n",
    "plt.ylabel(\"Max Value Change\")\n",
    "plt.title(\"Policy Evaluation Convergence\")\n",
    "plt.show()\n",
    "\n",
    "# ---- Plot expected return per policy improvement ----\n",
    "plt.figure()\n",
    "plt.plot(returns)\n",
    "plt.xlabel(\"Policy Iteration Step\")\n",
    "plt.ylabel(\"Sum of V(s)\")\n",
    "plt.title(\"Policy Improvement Progress\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf49fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bb9f83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
